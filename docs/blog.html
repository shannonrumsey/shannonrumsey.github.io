<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.330">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>shannonrumsey - Deep Dives</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./cv.html" rel="" target="">
 <span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./blog.html" rel="" target="" aria-current="page">
 <span class="menu-text">Deep Dives</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Resume.pdf" rel="" target="">
 <span class="menu-text">Resume</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/shannonrumsey" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/shannon-rumsey/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#gpt-architecture" id="toc-gpt-architecture" class="nav-link active" data-scroll-target="#gpt-architecture">GPT Architecture</a>
  <ul class="collapse">
  <li><a href="#an-analogy" id="toc-an-analogy" class="nav-link" data-scroll-target="#an-analogy">An Analogy</a></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#model-inputs" id="toc-model-inputs" class="nav-link" data-scroll-target="#model-inputs">Model Inputs</a>
  <ul class="collapse">
  <li><a href="#step-1-encoding" id="toc-step-1-encoding" class="nav-link" data-scroll-target="#step-1-encoding">Step 1: Encoding</a></li>
  <li><a href="#step-2-token-embedding-optimize-storing-word-informationmeaning" id="toc-step-2-token-embedding-optimize-storing-word-informationmeaning" class="nav-link" data-scroll-target="#step-2-token-embedding-optimize-storing-word-informationmeaning">Step 2: Token Embedding: Optimize Storing Word Information/Meaning</a></li>
  <li><a href="#step-3-positional-encodings" id="toc-step-3-positional-encodings" class="nav-link" data-scroll-target="#step-3-positional-encodings">Step 3: Positional Encodings</a></li>
  </ul></li>
  <li><a href="#transformer-block" id="toc-transformer-block" class="nav-link" data-scroll-target="#transformer-block">Transformer Block</a>
  <ul class="collapse">
  <li><a href="#step-4-attention-single-head" id="toc-step-4-attention-single-head" class="nav-link" data-scroll-target="#step-4-attention-single-head">Step 4: Attention (Single Head)</a></li>
  <li><a href="#step-5-multi-head-attention" id="toc-step-5-multi-head-attention" class="nav-link" data-scroll-target="#step-5-multi-head-attention">Step 5: Multi-head Attention</a></li>
  <li><a href="#step-6-feed-forward" id="toc-step-6-feed-forward" class="nav-link" data-scroll-target="#step-6-feed-forward">Step 6: Feed Forward</a></li>
  <li><a href="#step-7-decoding" id="toc-step-7-decoding" class="nav-link" data-scroll-target="#step-7-decoding">Step 7: Decoding</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Deep Dives</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="gpt-architecture" class="level1 page-columns page-full">
<h1>GPT Architecture</h1>
<section id="an-analogy" class="level3">
<h3 class="anchored" data-anchor-id="an-analogy">An Analogy</h3>
<p>We want to teach a computer the English language (which consists of 50,257 words) so it can communicate and assist in problem-solving. To begin, we might translate each English word into a computer-friendly format (e.g., cat -&gt; [0, 0, 1]). However, this approach is not very helpful - it‚Äôs as impractical as trying to learn Spanish using only an English-to-Spanish translation book. Instead relying on direct translations, we can describe each word with a set of traits or ratings (known as embeddings). These descriptions are continuously updated as the computer learns more about the language.</p>
<p>Since the order of words affects meaning (e.g., ‚ÄúThe cat chased the dog‚Äù vs.&nbsp;‚ÄúThe dog chased the cat‚Äù), we must tell the model where each word occurs in the sentence. One way to do this is by assigning each word a unique position using mathematical patterns, such as sine and cosine waves.</p>
<p>Imagine reading a sentence through different lenses. Each lens helps you focus on different aspects, such as relationships between subjects and objects, verb tense, or meaning. Multi-head attention does something similar by using multiple attention mechanisms in parallel to capture more complex relationships within the sentence.</p>
<p>Once attention identifies which words are most important, a feed forward network processes each word in more detail. It‚Äôs as if, after focusing on key parts of the sentence, you go back and annotate each word with additional context to deepen your understanding.</p>
<p>Finally, after the computer has gained a better understanding of the sentence, it translates its findings back into English.</p>
</section>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview</h3>
<p>GPT models generate text by predicting one word at a time based on the input sequence. They do this by assigning probabilities to potential next words and selecting the one with the highest probability.The model processes an input sequence of a fixed length (e.g., 2048 words for GPT-3), though shorter sequences are padded, and longer sequences are not permitted.</p>
</section>
<section id="model-inputs" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="model-inputs">Model Inputs</h2>
<section id="step-1-encoding" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="step-1-encoding">Step 1: Encoding</h3>

<div class="no-row-height column-margin column-container"><div class="">
<p>üí° Convert words into vectors</p>
</div></div><ul>
<li>Words are mapped to numeric values, forming a vocabulary (e.g., GPT-2 has a vocabulary size of 50,257 words).
<ul>
<li>Example: Cat = 1, Dog = 2, Car = 3, etc.</li>
</ul></li>
<li>Each word is expressed as a one-hot encoding vector (only zeros and ones) of size 50,257.
<ul>
<li>Example: Cat = [1, 0, 0, ‚Ä¶, 0], Dog = [0, 1, 0, ‚Ä¶, 0], Car = [0, 0, 1, ‚Ä¶, 0].</li>
</ul></li>
<li>Combining these vectors into a matrix, we end up with 2048 rows, each correpsonding to a token in the sequence, and 50,257 columns, each corresponding to a word in the dictionary.</li>
</ul>
</section>
<section id="step-2-token-embedding-optimize-storing-word-informationmeaning" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="step-2-token-embedding-optimize-storing-word-informationmeaning">Step 2: Token Embedding: Optimize Storing Word Information/Meaning</h3>

<div class="no-row-height column-margin column-container"><div class="">
<p>üí° Optimize storage and representation of word information</p>
</div></div><ul>
<li>Preserve semantic meaning of words in an optimal way using embedding functions.
<ul>
<li>One-hot encoding is sparse and inefficient.</li>
<li>We use embedding functions to project these vectors into a smaller, dense vector space.
<ul>
<li>Initialize an embedding matrix with dimensions [vocab size, num embedding].
<ul>
<li>Vocab size = 50,257, num embedding = the desired number of traits we want.</li>
</ul></li>
<li>Each one-hot vector is mapped to a dense embedding vector (via multiplication).
<ul>
<li>This results in an embedding vector of size [1, num embedding] for each word in the dictionary.</li>
</ul></li>
<li>Each element in the embedding represents a learned trait of the word.
<ul>
<li>Example: Dog might have an embedding embedding [energetic = 10, cute = 10], while Car has [energetic = 8, cute = 0].</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="step-3-positional-encodings" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="step-3-positional-encodings">Step 3: Positional Encodings</h3>

<div class="no-row-height column-margin column-container"><div class="">
<p>üí° Incorporate word order to enhance understanding of sentence meaning</p>
</div></div><ul>
<li>To model syntax and word order, we need positional embeddings.
<ul>
<li>Example: ‚ÄúIs the tea hot‚Äù differs from ‚ÄúThe tea is hot‚Äù.</li>
</ul></li>
<li>Two methods for positional encodings:
<ul>
<li>Sinusoidal Functions (Used in <em>Attention is All You Need</em>).
<ul>
<li>Encode positions using sine and cosine functions with different frequencies.
<ul>
<li>The frequency of the wave is tied to which bits we are representing.
<ul>
<li>A higher frequency allows us to represent the numbers 0-3. A lower frequency allows us to represent the numbers 4-15.</li>
</ul></li>
</ul></li>
<li>Each position is represented by a fixed vector that is consistent across different sequences/sentences. - The vector for the 5th position in one sentence is the same in all other sentences.</li>
</ul></li>
<li>Learned Positional Embeddings (Used in GPT-2).
<ul>
<li>Learn position-specific embeddings during backpropagation.</li>
<li>The position of words in a given sentence; not universal across all sentences.</li>
<li>Initialize a matrix with dimensions [input length, num embedding].
<ul>
<li>input length = the maximum sequence length that the model can expect (2048 words).</li>
<li>num embedding must be the same number used for the token embeddings because they will be summed together to form the official input of the model.</li>
</ul></li>
<li>What the matrix is filled with at initialization can vary.
<ul>
<li>PyTorch‚Äôs ‚Äúnn.Embedding‚Äù will randomly sample values from a standard normal distribution.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="transformer-block" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="transformer-block">Transformer Block</h2>
<section id="step-4-attention-single-head" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="step-4-attention-single-head">Step 4: Attention (Single Head)</h3>

<div class="no-row-height column-margin column-container"><div class="">
<p>üí° Determine the relevance of each word in the context of others</p>
</div></div><ul>
<li>Attention mechanisms evalulate the importance of each input token for predicting the output.</li>
<li>Each attention head processes a single word of the sequence at a time.</li>
<li>Each head computes three linear projections (query, key, and value) for each word
<ul>
<li>The resulting dimensions for each matrix are [batch size, input length, num embedding / number of heads].</li>
<li>Linear projection formula: y = Wx + b.
<ul>
<li>y = projection (query, key, and value).</li>
<li>W = weight matrix (distinct for query, key, and value).</li>
<li>x = input (sum of positional and token embeddings).</li>
<li>b = bias term.</li>
</ul></li>
</ul></li>
<li>Queries represent the task of interest.</li>
<li>Keys find relevant information, and values provide the actual token information.</li>
<li>The query and key matrices are multiplied and normalized through softmax to produce attention weights, which determine tokens most relevant to queries. - Example: In ‚Äúthe cat that chased the dog was tired,‚Äù ‚Äúthe cat‚Äù and ‚Äúwas tired‚Äù are related despite intermediate words.</li>
<li>The resulting attention weights are applied to the values matrix to gather relevant information about tokens.</li>
<li>The final resulting matrix is [batch size, input length, num embedding].</li>
</ul>
</section>
<section id="step-5-multi-head-attention" class="level3">
<h3 class="anchored" data-anchor-id="step-5-multi-head-attention">Step 5: Multi-head Attention</h3>
<ul>
<li>The attention process is repeated across multiple heads.</li>
<li>Results from each head are concatenated and transfromed linearly to match the original embedding dimension.
<ul>
<li>Ensures consistent dimensions ([batch size, input length, num embedding]).</li>
</ul></li>
</ul>
<p><strong>Step 5a: Residual Connection</strong></p>
<ul>
<li>Add the original input to the attention output.
<ul>
<li>Helps retain early-stage information and prevents vanishing gradients.</li>
</ul></li>
</ul>
<p><strong>Step 5b: Normalization</strong></p>
<ul>
<li>Normalize features for each token to have a mean of 0 and variance of 1 to stabilize training.</li>
<li>Layer normalization occurs after each attention layer in <em>Attention is All You Need</em>, while GPT-2 places it before each self-attention layer and after the final self-attention layer.</li>
</ul>
</section>
<section id="step-6-feed-forward" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="step-6-feed-forward">Step 6: Feed Forward</h3>

<div class="no-row-height column-margin column-container"><div class="">
<p>üí° Learn complex patterns and relationships</p>
</div></div><ul>
<li>A multi-layer perceptron with one hidden layer.</li>
<li>Processes inputs through hidden nodes to the output nodes.</li>
<li>Dimensions: [batch size, num embedding].</li>
<li>Each sequence is multiplied by a weight matrix and bias term (not to be confused with the weights matrices query, key, and value used in attention), then passed through an activation function to introduce non-linearity.
<ul>
<li>Non-linearity allows the network to model more complex patterns that cannot be captured by linear functions.</li>
</ul></li>
<li>The hidden layer often starts with a larger matrix (typically four times the original size) to capture more features, then is reduced to the original output size.</li>
<li>Uses backpropagation for training.
<ul>
<li>Backpropagation updates weights to produce better predictions and reduce error.
<ul>
<li>At each layer, backpropagation calculates how much the output of the layer (called activation) contributed to the overall loss by finding the derivative (called gradient) of the loss with respect to the layer‚Äôs outputs.</li>
<li>Based on the calculations, backpropagation calculates how much of each weight in the layer needs to be adjusted by finding the derivative (gradient) of the loss with respect to each weight.</li>
<li>Only the activation gradient is passed backward to the previous layer and the process of calculating the weights gradient is repeated.</li>
</ul></li>
</ul></li>
</ul>
<p><strong>Step 5a: Residual Connection</strong></p>
<ul>
<li>Add the original input to the attention output.
<ul>
<li>Helps retain early-stage information and prevents vanishing gradients.</li>
</ul></li>
</ul>
<p><strong>Step 5b: Normalization</strong></p>
<ul>
<li>Normalize features for each token to have a mean of 0 and variance of 1 to stabilize training.</li>
<li>Layer normalization occurs after each feed-forward layer in <em>Attention is All You Need</em>, while GPT-2 places it before each feed-forward layer.</li>
</ul>
</section>
<section id="step-7-decoding" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="step-7-decoding">Step 7: Decoding</h3>

<div class="no-row-height column-margin column-container"><div class="">
<p>üí° Convert vectors back into words</p>
</div></div><ul>
<li>After processing through all blocks, the final output is a matrix [sequence length, num embeddings].</li>
<li>Convert the matrix of vectors back into words using a linear transformation that scores each vocabulary word.
<ul>
<li>the weight matrix for this transformation is tied to the embedding layer to ensure that the representation of tokens is consistent throughout the process.</li>
<li>Scores are converted to probabilities using softmax.</li>
</ul></li>
<li>Top-k sampling can be used to determine the output.
<ul>
<li>Select the top k words based on scores.</li>
<li>Create probability distribution based on scores and randomly sample one word.</li>
</ul></li>
</ul>
<section id="references" class="level4">
<h4 class="anchored" data-anchor-id="references">References</h4>
<p><a href="https://dugas.ch/artificial_curiosity/GPT_architecture.html">The GPT-3 Architecture, on a Napkin</a></p>
<p><a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">Transformer Architecture: The Positional Encoding</a></p>
<p><a href="https://medium.com/mantisnlp/positional-encodings-i-main-approaches-bd1199d6770d">Positional Encodings: Main Approaches</a></p>
<p><a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">Feed Forward Neural Networks</a></p>
<p><a href="https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p">Top k and Top p</a></p>
</section>
<section id="a-code-implementation-can-be-found-here" class="level4">
<h4 class="anchored" data-anchor-id="a-code-implementation-can-be-found-here">A code implementation can be found <a href="https://github.com/shannonrumsey/medGPT">here</a></h4>


</section>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/shannonrumsey">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/shannon-rumsey/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>