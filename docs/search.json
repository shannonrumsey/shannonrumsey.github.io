[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shannon Rumsey",
    "section": "",
    "text": "I am currently a graduate student studying Natural Language Processing and am looking forward to a research oriented career that uses machine learning to create assistive technologies."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Education\nUniversity of California, Santa Barbara | B.S. in Statistics and Data Science, GPA: 3.63 (2021 - 2023)\nSanta Barbara City College | A.A. in Liberal Arts-Science: Science & Math, GPA: 3.86 (2019 - 2021)\n\n\nTechnical Skills\nProgramming: Python, R, SAS, SQL\nComputational Linguistics (Python): Neural Networks (DNN, RNN, LSTM), Sentiment Analysis, Natural Language Processing, Distributional Semantics, Automatic Summarization\nStatistics and Data Science (Python & R): Data Imputation Methods, Regression Analysis, Bayesian Data Analysis, Survival Analysis, Non-parametric Methods\n\n\nResearch Experience\n\nPost-baccalaureate Research Assistant\nUC Santa Barbara (June 2023 - Present)\nAs a post-baccalaureate research assistant for the CPLS Lab, I am conducting research that validates the Functional Load Hypothesis ‚Äì the theory that sounds distinguishing more words are less likely to disappear from a language or merge with other sounds. Using Word2Vec embeddings trained on the carefully selected corpora of nine languages, we are able to quantify the extent to which words differing by one sound, that is, minimal pairs, may be confused with each other in context and compared to a simple count of words. This important linguistic theory provides insight on how languages may evolve overtime.\nCPLS Lab\n\n\nCapstone Researcher\nEvidation Health, Inc.¬†(January 2023 - June 2023)\nThis project was a collaboration with Evidation to investigate alternative diagnostic metrics for respiratory viral infections (RVI). Using data collected from Fitbit devices, such as an individual‚Äôs resting heart rate and caloric expenditure, we designed a boosted tree model that was able to predict the outcome of the participant‚Äôs RVI lab test result and their hypothesized recovery interval. This project was presented at the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining in Long Beach, CA.\nAbstract Poster\n\n\nUndergraduate Data Science Fellow\nCentral Coast Data Science Partnership (September 2022 - June 2023)\nAs part of a cohort, I led outreach programs aimed at inspiring students and promoting departmental courses to a diverse student body. At the end of the academic year, I helped to organize and present at the Department of Probability and Statistics Project Showcase.\nCCDSP Profile\n\n\n\nOther Endeavors\n\nIndependent Researcher\nUC Santa Barbara (April 2023 - June 2023)\nUsing synonym replacement and text summarization, my team and I explored the methodology of medical transcription simplification and set out to define readability of documents. Our results indicated that our synonym replacement and summarization were successful in increasing the readability of medical transcriptions by reducing the Flesch-Kincaid metric, effectively democratizing information contained in these transcriptions. This project took on two phases: synonym simplification and summarization. In the first phase, we computed cosine similarities between BioBert embeddings derived from Harvard Health medical definitions and context dependent terminology found in the transcriptions to discover synonym substitutions. The second phase employed Bert2Bert to summarize and further refine the overall documents.\nAbstract\n\n\nIndependent Researcher (September 2022 - December 2022)\nWritten in the style of a follow-along blog post, I set out to assess the relationship between geographic attributes of a food-related business and their reviews. Referencing the Bing Sentiment Lexicon in R, I extracted text dependent information that encapsulated how reviewers felt beyond their star ratings. After extensive data collection on both the businesses and reviews, I trained 4 machine learning methods to identify key predictors for defining a business‚Äôs location within the Santa Barbara county. The number of reviews, review ratings, and votes on reviews proved to be the most important factors in a business‚Äôs rating, illuminating both the importance of business location but also the pitfalls of star ratings derived from averages.\nReport\n\n\n\nAwards\nHDR DSC Award #1924205 and Award #1924008, issued by the National Science Foundation‚Äôs Harnessing the Data Revolution Data Science Corps"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Shannon Rumsey",
    "section": "",
    "text": "University of California, Santa Cruz | Santa Cruz, CA M.S in Natural Language Processing | Sept 2024 - December 2025\nUniversity of California, Santa Barbara | Santa Barbara, CA B.S in Statistics and Data Science | Sept 2021 - December 2023"
  },
  {
    "objectID": "blog.html#references",
    "href": "blog.html#references",
    "title": "Deep Dive into GPT Architecture",
    "section": "References",
    "text": "References\n\nUnderstanding GPT Architecture\nTransformer Architecture and Positional Encoding\nPositional Encodings: Main Approaches\n\n\nSimple Summary\nWe want to teach a computer the English language (which consists of 50,257 words) so that it can communicate and aid in problem-solving. To begin, we translate each English word to the computer‚Äôs language equivalent (cat -&gt; [0, 0, 1]), but this is not very helpful because it‚Äôs impractical and can create meaningless sentences. It would be very difficult for a human to learn Spanish using only an English-to-Spanish translation book. Instead of a direct lookup, we can provide a description or set of ratings (an embedding) for each word. These descriptions/ratings are updated continuously as the computer understands more and more about the language.\nUnderstanding words is not enough to grasp a language; the way they are ordered is also important. For example, ‚Äúthe cat chased the mouse‚Äù has a very different meaning from ‚Äúthe mouse chased the cat‚Äù. In addition to providing a rating scheme for each word, we also include its position within the sentence.\nIn order for the computer to understand the language and communicate, it must be able to determine which words are important and how they relate to other words in the sentence. For example, in the sentence ‚Äúwhen she went to the bank, she saw a man,‚Äù the computer must be able to differentiate a financial institution from the side of a river, must reason that the two instances of ‚Äúshe‚Äù refer to the same person, and that the man was seen at the bank, not somewhere else.\n\n\nOverview\n\nGPT models take a sentence in and predict the next word (only produces one word/token at a time through the entire transformer!).\n\nThey do this by assigning probabilities to words it may be, and selecting the word with the highest probability.\nThe input length will always be a certain number of words (e.g., GPT-3‚Äôs is 2048 words).\n\nYou can have a smaller input length; there will just be padding (empty values).\nYou cannot have a larger input length.\n\n\n\n\n\nStep 1: Encoding\n\n\nüí° Turn words into vectors\n\nThere is a dictionary of all words. Each word is associated with a numeric value (e.g., GPT-2 has a vocab size of 50,257 words).\n\nCat = 1, Dog = 2, Car = 3, etc.\n\nFor each word, we map them to the corresponding location in the vocab dictionary, turning them into a one-hot encoding vector of size 50,257.\n\nCat would have the vector: [1, 0, 0, ‚Ä¶, 0].\nDog would have: [0, 1, 0, ‚Ä¶, 0].\nCar would have: [0, 0, 1, ‚Ä¶, 0].\n\nPutting these vectors together in a matrix, we end up with 2048 rows, representing the number of words/tokens in the input sequence, and 50,257 columns, representing the number of words in the dictionary. The row corresponds with the location in the sequence and the column represents the location in the dictionary.\n\nThe matrix is 2048 x 50,257 and is made up of only 0s and 1s.\n\n\n\n\nStep 2: Token Embedding: Optimize Storing Word Information/Meaning\n\n\nüí° Having so many empty (zeroes) slots in the matrix takes up too much unnecessary space\n\nPreserve semantic meaning of words in an optimal way using embedding functions.\n\nWe want to project this large one-hot encoded matrix to a smaller, dense vector space.\n\nWe initialize an embedding matrix with dimensions [vocab_size, embedding_dim].\n\nVocab size is 50,257 and embedding dim is the desired number of traits we want.\n\nEach one-hot vector in the matrix is mapped to this dense embedding vector (via multiplication).\n\nThis is called the embedding function.\nThis results in an embedding vector of size [1, embedding_dim] for each word in the dictionary.\n\nEach element in the embedding represents a learned trait of the word.\n\ni.e., Dog may have the vector embedding [energetic = 10, cute = 10], whereas car has [energetic = 8, cute = 0].\nThis is a better way of storing the data because it provides more semantic information and is in a lower dimension.\n\n\n\n\n\n\nStep 3: Positional Encodings\n\nWe need to take into account the syntax of the input sentence, which can affect the meaning of a sentence.\n\nFor example, ‚Äúthe tea is too hot, I am turning red‚Äù is different from ‚Äúthe tea is turning red, I am too hot.‚Äù\n\nWe have two options for learning position embeddings:\n\nEncode the position of the current token using sinusoidal functions with differing frequencies (Used in Attention is All You Need).\n\nThese are absolute/universal embeddings.\n\nEach position is represented with a fixed vector that is consistent across different sequences/sentences.\n\nThe vector for the 5th position in one sentence is the same in all other sentences.\n\n\nWe can represent the location number of a word in binary form, but it is more space-efficient to represent them using sinusoidal functions.\nThe frequency of the wave is tied to which bits we are representing.\n\nA higher frequency allows us to represent the numbers 0-3. A lower frequency allows us to represent the numbers 4-15.\nWe are able to do this because the lowest bit alternates on every number, the second on every two numbers, etc. We line up the length of the sine/cosine wave‚Äôs cycle with the bits we want to represent.\n\n\nTrained/Learned Positional Embeddings (Used in GPT-2).\n\nRelative positions of words.\n\nThe position of words in a given sentence; not universal across all sentences.\n\nInitialize a matrix with dimensions [max_positions, embedding_dim].\n\nWhat the matrix is filled with at initialization can vary. PyTorch‚Äôs ‚Äúnn.Embedding‚Äù will randomly sample values from a standard normal distribution.\nMax_positions is the maximum sequence length that the model can expect (2048 words).\nEmbedding_dim must be the same number used for the token embeddings because they will be summed together to form the official input of the model.\n\nDuring training, the position in the input sequence is mapped to its corresponding row in the matrix.\n\nThe second position in a sequence is the second row in the matrix.\nEmbeddings are learned during backpropagation and are adjusted to minimize the loss function.\nIn the forward pass, these learned positional embeddings are added to token embeddings.\n\n\n\n\n\n\nStep 4: Attention (Single Head)\n\nDetermine how important each input token is in predicting the output.\nEach attention head processes a single sequence at a time.\nFor each head, compute three linear projections to result in a query, key, and value projection. The resulting dimensions for each matrix are [batch size, sequence length, number of embeddings / number of heads].\n\nLinear projection: y = Wx + b.\n\ny is the resulting projection (query, key, and value).\nW is the matrix of weights.\n\nThe first weights matrix is query, the second for key, and the third for value.\n\nx is the input.\n\nThis is the summed positional embeddings and token embeddings.\n\nb is the bias.\n\nQueries are the task/meaning of interest.\nKeys find relevant information based on the task.\nValues are the actual information from the tokens.\n\nThe linear projection/transformation gives us 3 new projection matrices: queries, keys, and values.\n\nThe query and key (transposed) matrices are multiplied together and normalized through softmax (this turns raw scores into attention weights).\n\nDetermines tokens most relevant to queries.\nFor example, in ‚Äúthe cat that chased the dog was tired,‚Äù ‚Äúthe cat‚Äù and ‚Äúwas tired‚Äù are related despite there being words in between.\n\nThis new matrix is multiplied by the values matrix.\n\nProvides information about tokens of interest.\nThe final resulting matrix is [batch size, sequence length, number of embeddings].\n\n\n\n\n\nStep 5: Multi-head Attention\n\nThe process in step 4 is repeated multiple times (because of multiple heads).\nThe results from each attention head are concatenated together.\nA final linear transformation is applied to project the outputs back to the original embedding dimension.\n\nThis is important because it‚Äôs the dimensions ([batch size, sequence length, embedding dimensions]) that the layers expect as input.\n\n\n\n\nStep 6: Feed Forward"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Deep Dives",
    "section": "",
    "text": "We want to teach a computer the English language (which consists of 50,257 words) so it can communicate and assist in problem-solving. To begin, we might translate each English word into a computer-friendly format (e.g., cat -&gt; [0, 0, 1]). However, this approach is not very helpful - it‚Äôs as impractical as trying to learn Spanish using only an English-to-Spanish translation book. Instead relying on direct translations, we can describe each word with a set of traits or ratings (known as embeddings). These descriptions are continuously updated as the computer learns more about the language.\nSince the order of words affects meaning (e.g., ‚ÄúThe cat chased the dog‚Äù vs.¬†‚ÄúThe dog chased the cat‚Äù), we must tell the model where each word occurs in the sentence. One way to do this is by assigning each word a unique position using mathematical patterns, such as sine and cosine waves.\nImagine reading a sentence through different lenses. Each lens helps you focus on different aspects, such as relationships between subjects and objects, verb tense, or meaning. Multi-head attention does something similar by using multiple attention mechanisms in parallel to capture more complex relationships within the sentence.\nOnce attention identifies which words are most important, a feed forward network processes each word in more detail. It‚Äôs as if, after focusing on key parts of the sentence, you go back and annotate each word with additional context to deepen your understanding.\nFinally, after the computer has gained a better understanding of the sentence, it translates its findings back into English.\n\n\n\nGPT models generate text by predicting one word at a time based on the input sequence. They do this by assigning probabilities to potential next words and selecting the one with the highest probability.The model processes an input sequence of a fixed length (e.g., 2048 words for GPT-3), though shorter sequences are padded, and longer sequences are not permitted.\n\n\n\n\n\n\n\nüí° Convert words into vectors\n\nWords are mapped to numeric values, forming a vocabulary (e.g., GPT-2 has a vocabulary size of 50,257 words).\n\nExample: Cat = 1, Dog = 2, Car = 3, etc.\n\nEach word is expressed as a one-hot encoding vector (only zeros and ones) of size 50,257.\n\nExample: Cat = [1, 0, 0, ‚Ä¶, 0], Dog = [0, 1, 0, ‚Ä¶, 0], Car = [0, 0, 1, ‚Ä¶, 0].\n\nCombining these vectors into a matrix, we end up with 2048 rows, each correpsonding to a token in the sequence, and 50,257 columns, each corresponding to a word in the dictionary.\n\n\n\n\n\n\nüí° Optimize storage and representation of word information\n\nPreserve semantic meaning of words in an optimal way using embedding functions.\n\nOne-hot encoding is sparse and inefficient.\nWe use embedding functions to project these vectors into a smaller, dense vector space.\n\nInitialize an embedding matrix with dimensions [vocab size, num embedding].\n\nVocab size = 50,257, num embedding = the desired number of traits we want.\n\nEach one-hot vector is mapped to a dense embedding vector (via multiplication).\n\nThis results in an embedding vector of size [1, num embedding] for each word in the dictionary.\n\nEach element in the embedding represents a learned trait of the word.\n\nExample: Dog might have an embedding embedding [energetic = 10, cute = 10], while Car has [energetic = 8, cute = 0].\n\n\n\n\n\n\n\n\n\nüí° Incorporate word order to enhance understanding of sentence meaning\n\nTo model syntax and word order, we need positional embeddings.\n\nExample: ‚ÄúIs the tea hot‚Äù differs from ‚ÄúThe tea is hot‚Äù.\n\nTwo methods for positional encodings:\n\nSinusoidal Functions (Used in Attention is All You Need).\n\nEncode positions using sine and cosine functions with different frequencies.\n\nThe frequency of the wave is tied to which bits we are representing.\n\nA higher frequency allows us to represent the numbers 0-3. A lower frequency allows us to represent the numbers 4-15.\n\n\nEach position is represented by a fixed vector that is consistent across different sequences/sentences. - The vector for the 5th position in one sentence is the same in all other sentences.\n\nLearned Positional Embeddings (Used in GPT-2).\n\nLearn position-specific embeddings during backpropagation.\nThe position of words in a given sentence; not universal across all sentences.\nInitialize a matrix with dimensions [input length, num embedding].\n\ninput length = the maximum sequence length that the model can expect (2048 words).\nnum embedding must be the same number used for the token embeddings because they will be summed together to form the official input of the model.\n\nWhat the matrix is filled with at initialization can vary.\n\nPyTorch‚Äôs ‚Äúnn.Embedding‚Äù will randomly sample values from a standard normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nüí° Determine the relevance of each word in the context of others\n\nAttention mechanisms evalulate the importance of each input token for predicting the output.\nEach attention head processes a single word of the sequence at a time.\nEach head computes three linear projections (query, key, and value) for each word\n\nThe resulting dimensions for each matrix are [batch size, input length, num embedding / number of heads].\nLinear projection formula: y = Wx + b.\n\ny = projection (query, key, and value).\nW = weight matrix (distinct for query, key, and value).\nx = input (sum of positional and token embeddings).\nb = bias term.\n\n\nQueries represent the task of interest.\nKeys find relevant information, and values provide the actual token information.\nThe query and key matrices are multiplied and normalized through softmax to produce attention weights, which determine tokens most relevant to queries. - Example: In ‚Äúthe cat that chased the dog was tired,‚Äù ‚Äúthe cat‚Äù and ‚Äúwas tired‚Äù are related despite intermediate words.\nThe resulting attention weights are applied to the values matrix to gather relevant information about tokens.\nThe final resulting matrix is [batch size, input length, num embedding].\n\n\n\n\n\nThe attention process is repeated across multiple heads.\nResults from each head are concatenated and transfromed linearly to match the original embedding dimension.\n\nEnsures consistent dimensions ([batch size, input length, num embedding]).\n\n\nStep 5a: Residual Connection\n\nAdd the original input to the attention output.\n\nHelps retain early-stage information and prevents vanishing gradients.\n\n\nStep 5b: Normalization\n\nNormalize features for each token to have a mean of 0 and variance of 1 to stabilize training.\nLayer normalization occurs after each attention layer in Attention is All You Need, while GPT-2 places it before each self-attention layer and after the final self-attention layer.\n\n\n\n\n\n\nüí° Learn complex patterns and relationships\n\nA multi-layer perceptron with one hidden layer.\nProcesses inputs through hidden nodes to the output nodes.\nDimensions: [batch size, num embedding].\nEach sequence is multiplied by a weight matrix and bias term (not to be confused with the weights matrices query, key, and value used in attention), then passed through an activation function to introduce non-linearity.\n\nNon-linearity allows the network to model more complex patterns that cannot be captured by linear functions.\n\nThe hidden layer often starts with a larger matrix (typically four times the original size) to capture more features, then is reduced to the original output size.\nUses backpropagation for training.\n\nBackpropagation updates weights to produce better predictions and reduce error.\n\nAt each layer, backpropagation calculates how much the output of the layer (called activation) contributed to the overall loss by finding the derivative (called gradient) of the loss with respect to the layer‚Äôs outputs.\nBased on the calculations, backpropagation calculates how much of each weight in the layer needs to be adjusted by finding the derivative (gradient) of the loss with respect to each weight.\nOnly the activation gradient is passed backward to the previous layer and the process of calculating the weights gradient is repeated.\n\n\n\nStep 5a: Residual Connection\n\nAdd the original input to the attention output.\n\nHelps retain early-stage information and prevents vanishing gradients.\n\n\nStep 5b: Normalization\n\nNormalize features for each token to have a mean of 0 and variance of 1 to stabilize training.\nLayer normalization occurs after each feed-forward layer in Attention is All You Need, while GPT-2 places it before each feed-forward layer.\n\n\n\n\n\n\nüí° Convert vectors back into words\n\nAfter processing through all blocks, the final output is a matrix [sequence length, num embeddings].\nConvert the matrix of vectors back into words using a linear transformation that scores each vocabulary word.\n\nthe weight matrix for this transformation is tied to the embedding layer to ensure that the representation of tokens is consistent throughout the process.\nScores are converted to probabilities using softmax.\n\nTop-k sampling can be used to determine the output.\n\nSelect the top k words based on scores.\nCreate probability distribution based on scores and randomly sample one word.\n\n\n\n\nThe GPT-3 Architecture, on a Napkin\nTransformer Architecture: The Positional Encoding\nPositional Encodings: Main Approaches\nFeed Forward Neural Networks\nTop k and Top p"
  },
  {
    "objectID": "blog.html#gpt-architecture",
    "href": "blog.html#gpt-architecture",
    "title": "Deep Dives",
    "section": "",
    "text": "We want to teach a computer the English language (which consists of 50,257 words) so that it can communicate and aid in problem-solving. To begin, we translate each English word to the computer‚Äôs language equivalent (cat -&gt; [0, 0, 1]), but this is not very helpful because it‚Äôs impractical and can create meaningless sentences. It would be very difficult for a human to learn Spanish using only an English-to-Spanish translation book. Instead of a direct lookup, we can provide a description or set of ratings (an embedding) for each word. These descriptions/ratings are updated continuously as the computer understands more and more about the language.\nUnderstanding words is not enough to grasp a language; the way they are ordered is also important. For example, ‚Äúthe cat chased the mouse‚Äù has a very different meaning from ‚Äúthe mouse chased the cat‚Äù. In addition to providing a rating scheme for each word, we also include its position within the sentence.\nIn order for the computer to understand the language and communicate, it must be able to determine which words are important and how they relate to other words in the sentence. For example, in the sentence ‚Äúwhen she went to the bank, she saw a man,‚Äù the computer must be able to differentiate a financial institution from the side of a river, must reason that the two instances of ‚Äúshe‚Äù refer to the same person, and that the man was seen at the bank, not somewhere else.\n\n\n\n\nGPT models take a sentence in and predict the next word (only produces one word/token at a time through the entire transformer!).\n\nThey do this by assigning probabilities to words it may be, and selecting the word with the highest probability.\nThe input length will always be a certain number of words (e.g., GPT-3‚Äôs is 2048 words).\n\nYou can have a smaller input length; there will just be padding (empty values).\nYou cannot have a larger input length."
  },
  {
    "objectID": "blog.html#gpt-architecture-1",
    "href": "blog.html#gpt-architecture-1",
    "title": "Deep Dives",
    "section": "GPT Architecture",
    "text": "GPT Architecture\n\nAnalogy\nWe want to teach a computer the English language (which consists of 50,257 words) so that it can communicate and aid in problem-solving. To begin, we translate each English word to the computer‚Äôs language equivalent (cat -&gt; [0, 0, 1]), but this is not very helpful because it‚Äôs impractical and can create meaningless sentences. It would be very difficult for a human to learn Spanish using only an English-to-Spanish translation book. Instead of a direct lookup, we can provide a description or set of ratings (an embedding) for each word. These descriptions/ratings are updated continuously as the computer understands more and more about the language.\nUnderstanding words is not enough to grasp a language; the way they are ordered is also important. For example, ‚Äúthe cat chased the mouse‚Äù has a very different meaning from ‚Äúthe mouse chased the cat‚Äù. In addition to providing a rating scheme for each word, we also include its position within the sentence.\nIn order for the computer to understand the language and communicate, it must be able to determine which words are important and how they relate to other words in the sentence. For example, in the sentence ‚Äúwhen she went to the bank, she saw a man,‚Äù the computer must be able to differentiate a financial institution from the side of a river, must reason that the two instances of ‚Äúshe‚Äù refer to the same person, and that the man was seen at the bank, not somewhere else.\n\n\nOverview\n\nGPT models take a sentence in and predict the next word (only produces one word/token at a time through the entire transformer!).\n\nThey do this by assigning probabilities to words it may be, and selecting the word with the highest probability.\nThe input length will always be a certain number of words (e.g., GPT-3‚Äôs is 2048 words).\n\nYou can have a smaller input length; there will just be padding (empty values).\nYou cannot have a larger input length.\n\n\n\n\n\nStep 1: Encoding\n\n\nüí° Turn words into vectors\n\nThere is a dictionary of all words. Each word is associated with a numeric value (e.g., GPT-2 has a vocab size of 50,257 words).\n\nCat = 1, Dog = 2, Car = 3, etc.\n\nFor each word, we map them to the corresponding location in the vocab dictionary, turning them into a one-hot encoding vector of size 50,257.\n\nCat would have the vector: [1, 0, 0, ‚Ä¶, 0].\nDog would have: [0, 1, 0, ‚Ä¶, 0].\nCar would have: [0, 0, 1, ‚Ä¶, 0].\n\nPutting these vectors together in a matrix, we end up with 2048 rows, representing the number of words/tokens in the input sequence, and 50,257 columns, representing the number of words in the dictionary. The row corresponds with the location in the sequence and the column represents the location in the dictionary.\n\nThe matrix is 2048 x 50,257 and is made up of only 0s and 1s.\n\n\n\n\nStep 2: Token Embedding: Optimize Storing Word Information/Meaning\n\n\nüí° Having so many empty (zeroes) slots in the matrix takes up too much unnecessary space\n\nPreserve semantic meaning of words in an optimal way using embedding functions.\n\nWe want to project this large one-hot encoded matrix to a smaller, dense vector space.\n\nWe initialize an embedding matrix with dimensions [vocab_size, embedding_dim].\n\nVocab size is 50,257 and embedding dim is the desired number of traits we want.\n\nEach one-hot vector in the matrix is mapped to this dense embedding vector (via multiplication).\n\nThis is called the embedding function.\nThis results in an embedding vector of size [1, embedding_dim] for each word in the dictionary.\n\nEach element in the embedding represents a learned trait of the word.\n\ni.e., Dog may have the vector embedding [energetic = 10, cute = 10], whereas car has [energetic = 8, cute = 0].\nThis is a better way of storing the data because it provides more semantic information and is in a lower dimension.\n\n\n\n\n\n\nStep 3: Positional Encodings\n\nWe need to take into account the syntax of the input sentence, which can affect the meaning of a sentence.\n\nFor example, ‚Äúthe tea is too hot, I am turning red‚Äù is different from ‚Äúthe tea is turning red, I am too hot.‚Äù\n\nWe have two options for learning position embeddings:\n\nEncode the position of the current token using sinusoidal functions with differing frequencies (Used in Attention is All You Need).\n\nThese are absolute/universal embeddings.\n\nEach position is represented with a fixed vector that is consistent across different sequences/sentences.\n\nThe vector for the 5th position in one sentence is the same in all other sentences.\n\n\nWe can represent the location number of a word in binary form, but it is more space-efficient to represent them using sinusoidal functions.\nThe frequency of the wave is tied to which bits we are representing.\n\nA higher frequency allows us to represent the numbers 0-3. A lower frequency allows us to represent the numbers 4-15.\nWe are able to do this because the lowest bit alternates on every number, the second on every two numbers, etc. We line up the length of the sine/cosine wave‚Äôs cycle with the bits we want to represent.\n\n\nTrained/Learned Positional Embeddings (Used in GPT-2).\n\nRelative positions of words.\n\nThe position of words in a given sentence; not universal across all sentences.\n\nInitialize a matrix with dimensions [max_positions, embedding_dim].\n\nWhat the matrix is filled with at initialization can vary. PyTorch‚Äôs ‚Äúnn.Embedding‚Äù will randomly sample values from a standard normal distribution.\nMax_positions is the maximum sequence length that the model can expect (2048 words).\nEmbedding_dim must be the same number used for the token embeddings because they will be summed together to form the official input of the model.\n\nDuring training, the position in the input sequence is mapped to its corresponding row in the matrix.\n\nThe second position in a sequence is the second row in the matrix.\nEmbeddings are learned during backpropagation and are adjusted to minimize the loss function.\nIn the forward pass, these learned positional embeddings are added to token embeddings.\n\n\n\n\n\n\nStep 4: Attention (Single Head)\n\nDetermine how important each input token is in predicting the output.\nEach attention head processes a single sequence at a time.\nFor each head, compute three linear projections to result in a query, key, and value projection. The resulting dimensions for each matrix are [batch size, sequence length, number of embeddings / number of heads].\n\nLinear projection: y = Wx + b.\n\ny is the resulting projection (query, key, and value).\nW is the matrix of weights.\n\nThe first weights matrix is query, the second for key, and the third for value.\n\nx is the input.\n\nThis is the summed positional embeddings and token embeddings.\n\nb is the bias.\n\nQueries are the task/meaning of interest.\nKeys find relevant information based on the task.\nValues are the actual information from the tokens.\n\nThe linear projection/transformation gives us 3 new projection matrices: queries, keys, and values.\n\nThe query and key (transposed) matrices are multiplied together and normalized through softmax (this turns raw scores into attention weights).\n\nDetermines tokens most relevant to queries.\nFor example, in ‚Äúthe cat that chased the dog was tired,‚Äù ‚Äúthe cat‚Äù and ‚Äúwas tired‚Äù are related despite there being words in between.\n\nThis new matrix is multiplied by the values matrix.\n\nProvides information about tokens of interest.\nThe final resulting matrix is [batch size, sequence length, number of embeddings].\n\n\n\n\n\nStep 5: Multi-head Attention\n\nThe process in step 4 is repeated multiple times (because of multiple heads).\nThe results from each attention head are concatenated together.\nA final linear transformation is applied to project the outputs back to the original embedding dimension.\n\nThis is important because it‚Äôs the dimensions ([batch size, sequence length, embedding dimensions]) that the layers expect as input.\n\n\n\n\nStep 6: Feed Forward\n\nReferences\nUnderstanding GPT Architecture\nTransformer Architecture and Positional Encoding\nPositional Encodings: Main Approaches"
  },
  {
    "objectID": "blog.html#model-inputs",
    "href": "blog.html#model-inputs",
    "title": "Deep Dives",
    "section": "",
    "text": "üí° Convert words into vectors\n\nWords are mapped to numeric values, forming a vocabulary (e.g., GPT-2 has a vocabulary size of 50,257 words).\n\nExample: Cat = 1, Dog = 2, Car = 3, etc.\n\nEach word is expressed as a one-hot encoding vector (only zeros and ones) of size 50,257.\n\nExample: Cat = [1, 0, 0, ‚Ä¶, 0], Dog = [0, 1, 0, ‚Ä¶, 0], Car = [0, 0, 1, ‚Ä¶, 0].\n\nCombining these vectors into a matrix, we end up with 2048 rows, each correpsonding to a token in the sequence, and 50,257 columns, each corresponding to a word in the dictionary.\n\n\n\n\n\n\nüí° Optimize storage and representation of word information\n\nPreserve semantic meaning of words in an optimal way using embedding functions.\n\nOne-hot encoding is sparse and inefficient.\nWe use embedding functions to project these vectors into a smaller, dense vector space.\n\nInitialize an embedding matrix with dimensions [vocab size, num embedding].\n\nVocab size = 50,257, num embedding = the desired number of traits we want.\n\nEach one-hot vector is mapped to a dense embedding vector (via multiplication).\n\nThis results in an embedding vector of size [1, num embedding] for each word in the dictionary.\n\nEach element in the embedding represents a learned trait of the word.\n\nExample: Dog might have an embedding embedding [energetic = 10, cute = 10], while Car has [energetic = 8, cute = 0].\n\n\n\n\n\n\n\n\n\nüí° Incorporate word order to enhance understanding of sentence meaning\n\nTo model syntax and word order, we need positional embeddings.\n\nExample: ‚ÄúIs the tea hot‚Äù differs from ‚ÄúThe tea is hot‚Äù.\n\nTwo methods for positional encodings:\n\nSinusoidal Functions (Used in Attention is All You Need).\n\nEncode positions using sine and cosine functions with different frequencies.\n\nThe frequency of the wave is tied to which bits we are representing.\n\nA higher frequency allows us to represent the numbers 0-3. A lower frequency allows us to represent the numbers 4-15.\n\n\nEach position is represented by a fixed vector that is consistent across different sequences/sentences. - The vector for the 5th position in one sentence is the same in all other sentences.\n\nLearned Positional Embeddings (Used in GPT-2).\n\nLearn position-specific embeddings during backpropagation.\nThe position of words in a given sentence; not universal across all sentences.\nInitialize a matrix with dimensions [input length, num embedding].\n\ninput length = the maximum sequence length that the model can expect (2048 words).\nnum embedding must be the same number used for the token embeddings because they will be summed together to form the official input of the model.\n\nWhat the matrix is filled with at initialization can vary.\n\nPyTorch‚Äôs ‚Äúnn.Embedding‚Äù will randomly sample values from a standard normal distribution."
  },
  {
    "objectID": "blog.html#transformer-block",
    "href": "blog.html#transformer-block",
    "title": "Deep Dives",
    "section": "",
    "text": "üí° Determine the relevance of each word in the context of others\n\nAttention mechanisms evalulate the importance of each input token for predicting the output.\nEach attention head processes a single word of the sequence at a time.\nEach head computes three linear projections (query, key, and value) for each word\n\nThe resulting dimensions for each matrix are [batch size, input length, num embedding / number of heads].\nLinear projection formula: y = Wx + b.\n\ny = projection (query, key, and value).\nW = weight matrix (distinct for query, key, and value).\nx = input (sum of positional and token embeddings).\nb = bias term.\n\n\nQueries represent the task of interest.\nKeys find relevant information, and values provide the actual token information.\nThe query and key matrices are multiplied and normalized through softmax to produce attention weights, which determine tokens most relevant to queries. - Example: In ‚Äúthe cat that chased the dog was tired,‚Äù ‚Äúthe cat‚Äù and ‚Äúwas tired‚Äù are related despite intermediate words.\nThe resulting attention weights are applied to the values matrix to gather relevant information about tokens.\nThe final resulting matrix is [batch size, input length, num embedding].\n\n\n\n\n\nThe attention process is repeated across multiple heads.\nResults from each head are concatenated and transfromed linearly to match the original embedding dimension.\n\nEnsures consistent dimensions ([batch size, input length, num embedding]).\n\n\nStep 5a: Residual Connection\n\nAdd the original input to the attention output.\n\nHelps retain early-stage information and prevents vanishing gradients.\n\n\nStep 5b: Normalization\n\nNormalize features for each token to have a mean of 0 and variance of 1 to stabilize training.\nLayer normalization occurs after each attention layer in Attention is All You Need, while GPT-2 places it before each self-attention layer and after the final self-attention layer.\n\n\n\n\n\n\nüí° Learn complex patterns and relationships\n\nA multi-layer perceptron with one hidden layer.\nProcesses inputs through hidden nodes to the output nodes.\nDimensions: [batch size, num embedding].\nEach sequence is multiplied by a weight matrix and bias term (not to be confused with the weights matrices query, key, and value used in attention), then passed through an activation function to introduce non-linearity.\n\nNon-linearity allows the network to model more complex patterns that cannot be captured by linear functions.\n\nThe hidden layer often starts with a larger matrix (typically four times the original size) to capture more features, then is reduced to the original output size.\nUses backpropagation for training.\n\nBackpropagation updates weights to produce better predictions and reduce error.\n\nAt each layer, backpropagation calculates how much the output of the layer (called activation) contributed to the overall loss by finding the derivative (called gradient) of the loss with respect to the layer‚Äôs outputs.\nBased on the calculations, backpropagation calculates how much of each weight in the layer needs to be adjusted by finding the derivative (gradient) of the loss with respect to each weight.\nOnly the activation gradient is passed backward to the previous layer and the process of calculating the weights gradient is repeated.\n\n\n\nStep 5a: Residual Connection\n\nAdd the original input to the attention output.\n\nHelps retain early-stage information and prevents vanishing gradients.\n\n\nStep 5b: Normalization\n\nNormalize features for each token to have a mean of 0 and variance of 1 to stabilize training.\nLayer normalization occurs after each feed-forward layer in Attention is All You Need, while GPT-2 places it before each feed-forward layer.\n\n\n\n\n\n\nüí° Convert vectors back into words\n\nAfter processing through all blocks, the final output is a matrix [sequence length, num embeddings].\nConvert the matrix of vectors back into words using a linear transformation that scores each vocabulary word.\n\nthe weight matrix for this transformation is tied to the embedding layer to ensure that the representation of tokens is consistent throughout the process.\nScores are converted to probabilities using softmax.\n\nTop-k sampling can be used to determine the output.\n\nSelect the top k words based on scores.\nCreate probability distribution based on scores and randomly sample one word.\n\n\n\n\nThe GPT-3 Architecture, on a Napkin\nTransformer Architecture: The Positional Encoding\nPositional Encodings: Main Approaches\nFeed Forward Neural Networks\nTop k and Top p"
  }
]