---
title: "Deep Dives"
page-layout: full
---

# GPT Architecture

### An Analogy

We want to teach a computer the English language (which consists of 50,257 words) so it can communicate and assist in problem-solving. To begin, we might translate each English word into a computer-friendly format (e.g., cat -> [0, 0, 1]). However, this approach is not very helpful - it's as impractical as trying to learn Spanish using only an English-to-Spanish translation book. Instead relying on direct translations, we can describe each word with a set of traits or ratings (known as embeddings). These descriptions are continuously updated as the computer learns more about the language. 

Since the order of words affects meaning (e.g., "The cat chased the dog" vs. "The dog chased the cat"), we must tell the model where each word occurs in the sentence. One way to do this is by assigning each word a unique position using mathematical patterns, such as sine and cosine waves.

Imagine reading a sentence through different lenses. Each lens helps you focus on different aspects, such as relationships between subjects and objects, verb tense, or meaning. Multi-head attention does something similar by using multiple attention mechanisms in parallel to capture more complex relationships within the sentence.

Once attention identifies which words are most important, a feed forward network processes each word in more detail. It's as if, after focusing on key parts of the sentence, you go back and annotate each word with additional context to deepen your understanding.

Finally, after the computer has gained a better understanding of the sentence, it translates its findings back into English.


### Overview

GPT models generate text by predicting one word at a time based on the input sequence. They do this by assigning probabilities to potential next words and selecting the one with the highest probability.The model processes an input sequence of a fixed length (e.g., 2048 words for GPT-3), though shorter sequences are padded, and longer sequences are not permitted.

## Model Inputs

### Step 1: Encoding
::: aside
üí° Convert words into vectors
:::

- Words are mapped to numeric values, forming a vocabulary (e.g., GPT-2 has a vocabulary size of 50,257 words).
    - Example: Cat = 1, Dog = 2, Car = 3, etc.
- Each word is expressed as a one-hot encoding vector (only zeros and ones) of size 50,257.
    - Example: Cat = \[1, 0, 0, ‚Ä¶, 0\], Dog =  \[0, 1, 0, ‚Ä¶, 0\], Car = \[0, 0, 1, ‚Ä¶, 0\].
-   Combining these vectors into a matrix, we end up with 2048 rows, each correpsonding to a token in the sequence, and 50,257 columns, each corresponding to a word in the dictionary.

### Step 2: Token Embedding: Optimize Storing Word Information/Meaning

::: aside
üí° Optimize storage and representation of word information
:::

- Preserve semantic meaning of words in an optimal way using embedding functions.
    - One-hot encoding is sparse and inefficient.
    - We use embedding functions to project these vectors into a smaller, dense vector space.
        - Initialize an embedding matrix with dimensions \[vocab size, num embedding\].
            - Vocab size = 50,257, num embedding = the desired number of traits we want.
        - Each one-hot vector is mapped to a dense embedding vector (via multiplication).
            - This results in an embedding vector of size \[1, num embedding\] for each word in the dictionary.
        - Each element in the embedding represents a learned trait of the word.
            - Example: Dog might have an embedding embedding \[energetic = 10, cute = 10\], while Car has \[energetic = 8, cute = 0\].

### Step 3: Positional Encodings
::: aside
üí° Incorporate word order to enhance understanding of sentence meaning
:::
- To model syntax and word order, we need positional embeddings.
    - Example: ‚ÄúIs the tea hot‚Äù differs from ‚ÄúThe tea is hot‚Äù.
- Two methods for positional encodings:
    - Sinusoidal Functions (Used in *Attention is All You Need*).
        - Encode positions using sine and cosine functions with different frequencies.
            - The frequency of the wave is tied to which bits we are representing.
                - A higher frequency allows us to represent the numbers 0-3. A lower frequency allows us to represent the numbers 4-15.
        - Each position is represented by a fixed vector that is consistent across different sequences/sentences.
                - The vector for the 5th position in one sentence is the same in all other sentences.

    - Learned Positional Embeddings (Used in GPT-2).
        - Learn position-specific embeddings during backpropagation.
        - The position of words in a given sentence; not universal across all sentences.
        - Initialize a matrix with dimensions \[input length, num embedding\].
            - input length = the maximum sequence length that the model can expect (2048 words).
            - num embedding must be the same number used for the token embeddings because they will be summed together to form the official input of the model.
        - What the matrix is filled with at initialization can vary. 
            - PyTorch‚Äôs ‚Äúnn.Embedding‚Äù will randomly sample values from a standard normal distribution.
           
## Transformer Block

### Step 4: Attention (Single Head)
::: aside
üí° Determine the relevance of each word in the context of others
:::
- Attention mechanisms evalulate the importance of each input token for predicting the output.
-   Each attention head processes a single word of the sequence at a time.
- Each head computes three linear projections (query, key, and value) for each word 
   -  The resulting dimensions for each matrix are \[batch size, input length, num embedding / number of heads\].
    - Linear projection formula: y = Wx + b.
        - y = projection (query, key, and value).
        - W = weight matrix (distinct for query, key, and value).
        - x = input (sum of positional and token embeddings).
        - b = bias term.
- Queries represent the task of interest.
- Keys find relevant information, and values provide the actual token information.
- The query and key matrices are multiplied and normalized through softmax to produce attention weights, which determine tokens most relevant to queries.
        - Example: In ‚Äúthe cat that chased the dog was tired,‚Äù ‚Äúthe cat‚Äù and ‚Äúwas tired‚Äù are related despite intermediate words.
- The resulting attention weights are applied to the values matrix to gather relevant information about tokens.
- The final resulting matrix is \[batch size, input length, num embedding\].

### Step 5: Multi-head Attention

- The attention process is repeated across multiple heads.
- Results from each head are concatenated and transfromed linearly to match the original embedding dimension.
    - Ensures consistent dimensions (\[batch size, input length, num embedding\]).
    
**Step 5a: Residual Connection**

- Add the original input to the attention output.
    - Helps retain early-stage information and prevents vanishing gradients.

**Step 5b: Normalization**

- Normalize features for each token to have a mean of 0 and variance of 1 to stabilize training.
- Layer normalization occurs after each attention layer in *Attention is All You Need*, while GPT-2 places it before each self-attention layer and after the final self-attention layer.

### Step 6: Feed Forward
::: aside
üí° Learn complex patterns and relationships
:::

- A multi-layer perceptron with one hidden layer.
- Processes inputs through hidden nodes to the output nodes.
- Dimensions: [batch size, num embedding].
- Each sequence is multiplied by a weight matrix and bias term (not to be confused with the weights matrices query, key, and value used in attention), then passed through an activation function to introduce non-linearity.
    - Non-linearity allows the network to model more complex patterns that cannot be captured by linear functions.
- The hidden layer often starts with a larger matrix (typically four times the original size) to capture more features, then is reduced to the original output size.
- Uses backpropagation for training.
    - Backpropagation updates weights to produce better predictions and reduce error.
        - At each layer, backpropagation calculates how much the output of the layer (called activation) contributed to the overall loss by finding the derivative (called gradient) of the loss with respect to the layer‚Äôs outputs.
        - Based on the calculations, backpropagation calculates how much of each weight in the layer needs to be adjusted by finding the derivative (gradient) of the loss with respect to each weight.
        - Only the activation gradient is passed backward to the previous layer and the process of calculating the weights gradient is repeated.

**Step 5a: Residual Connection**

- Add the original input to the attention output.
    - Helps retain early-stage information and prevents vanishing gradients.

**Step 5b: Normalization**

- Normalize features for each token to have a mean of 0 and variance of 1 to stabilize training.
- Layer normalization occurs after each feed-forward layer in *Attention is All You Need*, while GPT-2 places it before each feed-forward layer.

### Step 7: Decoding
::: aside
üí° Convert vectors back into words
:::

- After processing through all blocks, the final output is a matrix [sequence length, num embeddings].
- Convert the matrix of vectors back into words using a  linear transformation that scores each vocabulary word.
    - the weight matrix for this transformation is tied to the embedding layer to ensure that the representation of tokens is consistent throughout the process.
    - Scores are converted to probabilities using softmax.
- Top-k sampling can be used to determine the output.
    - Select the top k words based on scores.
    - Create probability distribution based on scores and randomly sample one word.
    
    
#### References

[The GPT-3 Architecture, on a Napkin](https://dugas.ch/artificial_curiosity/GPT_architecture.html)

[Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)

[Positional Encodings: Main Approaches](https://medium.com/mantisnlp/positional-encodings-i-main-approaches-bd1199d6770d)

[Feed Forward Neural Networks](https://en.wikipedia.org/wiki/Feedforward_neural_network)

[Top k and Top p](https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p)

#### A code implementation can be found [here](https://github.com/shannonrumsey/medGPT)