---
title: "Deep Dives"
page-layout: full
---

# GPT Architecture

### An Analogy

We want to teach a computer the English language (which consists of 50,257 words) so that it can communicate and aid in problem-solving. To begin, we translate each English word to the computer‚Äôs language equivalent (cat -\> \[0, 0, 1\]), but this is not very helpful because it‚Äôs impractical and can create meaningless sentences. It would be very difficult for a human to learn Spanish using only an English-to-Spanish translation book. Instead of a direct lookup, we can provide a description or set of ratings (an embedding) for each word. These descriptions/ratings are updated continuously as the computer understands more and more about the language.

Understanding words is not enough to grasp a language; the way they are ordered is also important. For example, ‚Äúthe cat chased the mouse‚Äù has a very different meaning from ‚Äúthe mouse chased the cat‚Äù. In addition to providing a rating scheme for each word, we also include its position within the sentence.

In order for the computer to understand the language and communicate, it must be able to determine which words are important and how they relate to other words in the sentence. For example, in the sentence ‚Äúwhen she went to the bank, she saw a man,‚Äù the computer must be able to differentiate a financial institution from the side of a river, must reason that the two instances of ‚Äúshe‚Äù refer to the same person, and that the man was seen at the bank, not somewhere else.



### Overview

-   GPT models take a sentence in and predict the next word (only produces one word/token at a time through the entire transformer!).
    -   They do this by assigning probabilities to words it may be, and selecting the word with the highest probability.
    -   The input length will always be a certain number of words (e.g., GPT-3‚Äôs is 2048 words).
        -   You can have a smaller input length; there will just be padding (empty values).
        -   You cannot have a larger input length.

## Model Inputs

### Step 1: Encoding
::: aside
üí° Turn words into vectors
:::

-   There is a dictionary of all words. Each word is associated with a numeric value (e.g., GPT-2 has a vocab size of 50,257 words).
    -   Cat = 1, Dog = 2, Car = 3, etc.
-   For each word, we map them to the corresponding location in the vocab dictionary, turning them into a one-hot encoding vector of size 50,257.
    -   Cat would have the vector: \[1, 0, 0, ‚Ä¶, 0\].
    -   Dog would have: \[0, 1, 0, ‚Ä¶, 0\].
    -   Car would have: \[0, 0, 1, ‚Ä¶, 0\].
-   Putting these vectors together in a matrix, we end up with 2048 rows, representing the number of words/tokens in the input sequence, and 50,257 columns, representing the number of words in the dictionary. The row corresponds with the location in the sequence and the column represents the location in the dictionary.
    -   The matrix is 2048 x 50,257 and is made up of only 0s and 1s.

### Step 2: Token Embedding: Optimize Storing Word Information/Meaning

::: aside
üí° Having so many empty (zeroes) slots in the matrix takes up too much unnecessary space
:::

-   Preserve semantic meaning of words in an optimal way using embedding functions.
    -   We want to project this large one-hot encoded matrix to a smaller, dense vector space.
        -   We initialize an embedding matrix with dimensions \[vocab_size, embedding_dim\].
            -   Vocab size is 50,257 and embedding dim is the desired number of traits we want.
        -   Each one-hot vector in the matrix is mapped to this dense embedding vector (via multiplication).
            -   This is called the embedding function.
            -   This results in an embedding vector of size \[1, embedding_dim\] for each word in the dictionary.
        -   Each element in the embedding represents a learned trait of the word.
            -   i.e., Dog may have the vector embedding \[energetic = 10, cute = 10\], whereas car has \[energetic = 8, cute = 0\].
            -   This is a better way of storing the data because it provides more semantic information and is in a lower dimension.

### Step 3: Positional Encodings
::: aside
üí° Represent the order of words in a sentence to further model understanding of sentence meaning
:::
-   We need to take into account the syntax of the input sentence, which can affect the meaning of a sentence.
    -   For example, ‚Äúthe tea is too hot, I am turning red‚Äù is different from ‚Äúthe tea is turning red, I am too hot.‚Äù
-   We have two options for learning position embeddings:
    -   Encode the position of the current token using sinusoidal functions with differing frequencies (Used in *Attention is All You Need*).
        -   These are absolute/universal embeddings.
            -   Each position is represented with a fixed vector that is consistent across different sequences/sentences.
                -   The vector for the 5th position in one sentence is the same in all other sentences.
        -   We can represent the location number of a word in binary form, but it is more space-efficient to represent them using sinusoidal functions.
        -   The frequency of the wave is tied to which bits we are representing.
            -   A higher frequency allows us to represent the numbers 0-3. A lower frequency allows us to represent the numbers 4-15.
            -   We are able to do this because the lowest bit alternates on every number, the second on every two numbers, etc. We line up the length of the sine/cosine wave‚Äôs cycle with the bits we want to represent.
    -   Trained/Learned Positional Embeddings (Used in GPT-2).
        -   Relative positions of words.
            -   The position of words in a given sentence; not universal across all sentences.
        -   Initialize a matrix with dimensions \[max_positions, embedding_dim\].
            -   What the matrix is filled with at initialization can vary. PyTorch‚Äôs ‚Äúnn.Embedding‚Äù will randomly sample values from a standard normal distribution.
            -   Max_positions is the maximum sequence length that the model can expect (2048 words).
            -   Embedding_dim must be the same number used for the token embeddings because they will be summed together to form the official input of the model.
        -   During training, the position in the input sequence is mapped to its corresponding row in the matrix.
            -   The second position in a sequence is the second row in the matrix.
            -   Embeddings are learned during backpropagation and are adjusted to minimize the loss function.
            -   In the forward pass, these learned positional embeddings are added to token embeddings.

## Transformer Block

### Step 4: Attention (Single Head)
::: aside
üí° Produce an attention matrix which determines how important each word is in relation to the others
:::
-   Determine how important each input token is in predicting the output.
-   Each attention head processes a single sequence at a time.
-   For each head, compute three linear projections to result in a query, key, and value projection. The resulting dimensions for each matrix are \[batch size, sequence length, number of embeddings / number of heads\].
    -   Linear projection: y = Wx + b.
        -   y is the resulting projection (query, key, and value).
        -   W is the matrix of weights.
            -   The first weights matrix is query, the second for key, and the third for value.
        -   x is the input.
            -   This is the summed positional embeddings and token embeddings.
        -   b is the bias.
    -   Queries are the task/meaning of interest.
    -   Keys find relevant information based on the task.
    -   Values are the actual information from the tokens.
-   The linear projection/transformation gives us 3 new projection matrices: queries, keys, and values.
    -   The query and key (transposed) matrices are multiplied together and normalized through softmax (this turns raw scores into attention weights).
        -   Determines tokens most relevant to queries.
        -   For example, in ‚Äúthe cat that chased the dog was tired,‚Äù ‚Äúthe cat‚Äù and ‚Äúwas tired‚Äù are related despite there being words in between.
    -   This new matrix is multiplied by the values matrix.
        -   Provides information about tokens of interest.
        -   The final resulting matrix is \[batch size, sequence length, number of embeddings\].

### Step 5: Multi-head Attention

-   The process in step 4 is repeated multiple times (because of multiple heads).
-   The results from each attention head are concatenated together.
-   A final linear transformation is applied to project the outputs back to the original embedding dimension.
    -   This is important because it‚Äôs the dimensions (\[batch size, sequence length, embedding dimensions\]) that the layers expect as input.
    
**Step 5a: Residual Connection**

- The inputs to this layer are added (simple addition) to the resulting outputs
    - this helps the model retain information from earlier stages of the network and prevents vanishing gradients

**Step 5b: Normalization**

- For each token in a sequence, we calculate the mean and variance across its features and normalize them so that the features have a mean of 0 and a variance of 1. By doing this, we are able to stabilize the training process
- In the Attention is All You Need paper, layer normalization occurs after each attention layer
- In GPT-2, layer normalization occurs before each self-attention layer in the transformer and after the final self-attention layer

### Step 6: Feed Forward
::: aside
üí° This is where the model learns complex patterns and relationships
:::

- This is a multi-layer perceptron with one hidden layer
- Information (embeddings) flows from the input nodes, through the hidden nodes, to the output nodes
- Dimensions of inputs: [batch size, number of embeddings]
- Each sequence in the batch is multiplied by the weights matrix (not to be confused with the weights matrices query, key, and value used in attention) and combined with a learned bias term
- The sequence is then passed through an activation function to introduce non-linearity
    - non-linearity is important because it allows the network to model more complex patterns that cannot be captured by linear functions
- This process is repeated through the hidden layer to get the final result
    - The hidden layer starts with a larger matrix (typically four times the original size) to capture and learn a broader set of features and relationships from the data. However, after processing in the hidden layer, it is refined and reduced, often matching the input shape or desired output shape
- These types of neural networks are trained using backpropagation
    - Backpropagation calculates (updates) exactly how much each weight should change in order for the neural network to produce better predictions and reduce error
        - It starts at the last layer and moves up towards the first layer
            - At each layer, backpropagation calculates how much the output of the layer (called activation) contributed to the overall loss by finding the derivative (called gradient) of the loss with respect to the layer‚Äôs outputs.
            - Based on the calculations, backpropagation calculates how much of each weight in the layer needs to be adjusted by finding the derivative (gradient) of the loss with respect to each weight.
            - Only the activation gradient is passed backward to the previous layer and the process of calculating the weights gradient is repeated.

**Step 6a: Residual Connection**

- The inputs to this layer are added (simple addition) to the resulting outputs
    - this helps the model retain information from earlier stages of the network and prevents vanishing gradients

**Step 6b: Normalization**

- For each token in a sequence, we calculate the mean and variance across its features and normalize them so that the features have a mean of 0 and a variance of 1. By doing this, we are able to stabilize the training process
- In the Attention is All You Need paper, layer normalization occurs after each feed forward layer
- In GPT-2, layer normalization occurs before each feed forward layer in the transformer

### Step 7: Decoding
::: aside
üí° Turn vectors into words
:::

- After each block (composed of self attention and feed forward) are repeated the desired number of times, we end up with a matrix [sequence length, number of embeddings]
    - Each position in the sequence length has an associated vector of embeddings that describes that particular position in the sequence
- In order to convert the matrix of vectors back into words, we reverse the embedding mapping
    - this is done using a linear transformation which gives us a score for each word in the vocabulary for each output position
    - the weight matrix used in the linear transformation for generating vocabulary scores is the same (tied) as the weight matrix used in the embedding layer (at the very beginning of the model)
        - this ensures that the representations of tokens used in the model are consistent throughout the process
    - the scores are converted into probabilities using softmax, where the probability represents how likely a word is the correct one for that position
- Top-k sampling can be used to determine the final word
    - the first k highest scored words are selected from the vocabulary
    - a probability distribution of the k words is created based on their scores and the and we randomly sample one word from the distribution
    
    
#### References

[The GPT-3 Architecture, on a Napkin](https://dugas.ch/artificial_curiosity/GPT_architecture.html)

[Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)

[Positional Encodings: Main Approaches](https://medium.com/mantisnlp/positional-encodings-i-main-approaches-bd1199d6770d)

[Feed Forward Neural Networks](https://en.wikipedia.org/wiki/Feedforward_neural_network)

[Top k and Top p](https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p)

#### An implementation following [this tutorial](https://www.youtube.com/watch?v=l8pRSuU81PU&t=4895s) can be found [here](https://github.com/shannonrumsey/medGPT)